{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c6408-8607-4178-8f5b-e0d7b4157264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "import torchvision.models as models\n",
    "from pathlib import Path # Use pathlib for better path handling\n",
    "import albumentations as A # For data augmentation\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import warnings\n",
    "import time\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# --- Constants ---\n",
    "# Updated for B3 dataset with 9 keypoints\n",
    "KEYPOINTS_NAMES = [\"wither\", \"pinbone\", \"shoulderbone\", \"front_girth_bottom\", \"front_girth_top\", \n",
    "                   \"Height_bottom\", \"Height_top\", \"rear_girth_bottom\", \"rear_girth_top\"]\n",
    "NUM_KEYPOINTS = len(KEYPOINTS_NAMES)  # Now 9 instead of 6\n",
    "TARGET_SIZE = (224, 224) # Standard size for many pre-trained models\n",
    "BATCH_SIZE = 64 # Adjusted batch size (tune based on GPU memory)\n",
    "EPOCHS = 100 # Adjusted epochs (tune based on convergence)\n",
    "LEARNING_RATE = 0.004 # Adjusted learning rate (tune)\n",
    "WEIGHT_DECAY = 0.01 # Weight decay for AdamW\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ImageNet mean/std for normalization with pre-trained models\n",
    "SEGMENTATION_INPUT_SIZE = (512, 512)\n",
    "NORM_MEAN = [0.485, 0.456, 0.406]\n",
    "NORM_STD = [0.229, 0.224, 0.225]\n",
    "KERAS_MODEL_PATH = r'best_keypoints_model_9pts_limited.keras' # Path to your pre-trained Keras keypoint model\n",
    "BEST_MODEL_SAVE_PATH = r'best_enhanced_triple_cattle_weight_model_res18_100ep_2600p.pth'\n",
    "BEST_TRAIN_MODEL = r'best_enhanced_triple_cattle_weight_model_res18_100ep_2600p.pth'\n",
    "SEGMENTATION_MODEL_PATH = r'best_cattle_segmentation_model_6.keras'\n",
    "\n",
    "\n",
    "# --- Helper Function to Create Segmentation Model ---\n",
    "def create_model(num_classes=3):\n",
    "    \"\"\"Create a UNet model with a ResNet18 backbone\"\"\"\n",
    "    try:\n",
    "        model = smp.Unet(\n",
    "            encoder_name=\"resnet18\",\n",
    "            encoder_weights=\"imagenet\",\n",
    "            in_channels=3,\n",
    "            classes=num_classes,\n",
    "        )\n",
    "        print(\"Segmentation model created successfully\")\n",
    "        print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating segmentation model: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- Keras Model Loading (for inference keypoint prediction) ---\n",
    "# Note: This introduces a dependency on TensorFlow/Keras for the inference part\n",
    "# If you replace keypoint prediction with a PyTorch model or use ground truth, this can be removed.\n",
    "try:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    if Path(KERAS_MODEL_PATH).is_file():\n",
    "        keypoint_detector_model = load_model(KERAS_MODEL_PATH)\n",
    "        print(f\"Successfully loaded Keras keypoint model from {KERAS_MODEL_PATH}\")\n",
    "    else:\n",
    "        keypoint_detector_model = None\n",
    "        warnings.warn(f\"Keras keypoint model not found at {KERAS_MODEL_PATH}. \"\n",
    "                      f\"`get_keypoints_from_image` will return None.\")\n",
    "except ImportError:\n",
    "    warnings.warn(\"TensorFlow/Keras not installed. `get_keypoints_from_image` will not function.\")\n",
    "    keypoint_detector_model = None\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"Error loading Keras model from {KERAS_MODEL_PATH}: {e}\")\n",
    "    keypoint_detector_model = None\n",
    "\n",
    "\n",
    "# --- PyTorch Segmentation Model Loading ---\n",
    "try:\n",
    "    segmentation_model = create_model(num_classes=3)\n",
    "    if Path(SEGMENTATION_MODEL_PATH).is_file():\n",
    "        # Correctly load the state dictionary for a PyTorch model\n",
    "        segmentation_model.load_state_dict(torch.load(SEGMENTATION_MODEL_PATH, map_location=DEVICE))\n",
    "        segmentation_model.to(DEVICE)\n",
    "        segmentation_model.eval()  # Set to evaluation mode\n",
    "        print(f\"Successfully loaded PyTorch segmentation model from {SEGMENTATION_MODEL_PATH}\")\n",
    "    else:\n",
    "        segmentation_model = None\n",
    "        warnings.warn(f\"Segmentation model not found at {SEGMENTATION_MODEL_PATH}. Segmentation will not be available.\")\n",
    "except Exception as e:\n",
    "    warnings.warn(f\"Error loading segmentation model: {e}\")\n",
    "    segmentation_model = None\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def find_segmented_filename(original_filename: str, segmented_dir: Path) -> str | None:\n",
    "    \"\"\"Find the corresponding segmented image by matching the base part of the filename\"\"\"\n",
    "    base_name = Path(original_filename).stem # Get filename without extension\n",
    "    # Look for files starting with the base name (more robust)\n",
    "    for f in segmented_dir.glob(f\"{base_name}*\"):\n",
    "        return f.name # Return the filename string\n",
    "    # Fallback: try matching just the prefix if separated by '.' or '_'\n",
    "    parts = base_name.split('_')\n",
    "    if len(parts) > 1:\n",
    "       prefix = parts[0]\n",
    "       for f in segmented_dir.glob(f\"{prefix}*\"):\n",
    "           return f.name\n",
    "    parts = base_name.split('.')\n",
    "    if len(parts) > 1:\n",
    "       prefix = parts[0]\n",
    "       for f in segmented_dir.glob(f\"{prefix}*\"):\n",
    "           return f.name\n",
    "    return None\n",
    "\n",
    "def load_keypoints_data(json_annotation_path: Path) -> dict[str, list[float]]:\n",
    "    \"\"\"Load keypoints data (x, y) from COCO-style JSON annotation file\"\"\"\n",
    "    if not json_annotation_path.is_file():\n",
    "        raise FileNotFoundError(f\"Annotation file not found: {json_annotation_path}\")\n",
    "    with open(json_annotation_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    keypoints_data = {}\n",
    "    image_id_to_filename = {img['id']: img['file_name'] for img in data['images']}\n",
    "\n",
    "    # Debug info to help understand the data structure\n",
    "    print(f\"JSON contains {len(data.get('images', []))} images and {len(data.get('annotations', []))} annotations\")\n",
    "    \n",
    "    if 'categories' in data:\n",
    "        for cat in data['categories']:\n",
    "            if 'keypoints' in cat:\n",
    "                print(f\"Category {cat['name']} has {len(cat['keypoints'])} keypoints: {cat['keypoints']}\")\n",
    "    \n",
    "    # Check if NUM_KEYPOINTS matches the data\n",
    "    expected_keypoints_length = NUM_KEYPOINTS * 3  # x, y, visibility for each keypoint\n",
    "    \n",
    "    for annotation in data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        if image_id in image_id_to_filename:\n",
    "            file_name = image_id_to_filename[image_id]\n",
    "            keypoints = annotation['keypoints'] # Format: [x1, y1, v1, x2, y2, v2, ...]\n",
    "            \n",
    "            # Debug for first few annotations to understand format\n",
    "            if len(keypoints_data) < 5:\n",
    "                print(f\"Sample keypoints for {file_name}: {keypoints[:6]}...\")\n",
    "            \n",
    "            # Check if keypoints length matches our expectation\n",
    "            if len(keypoints) != expected_keypoints_length:\n",
    "                print(f\"Warning: {file_name} has {len(keypoints)} values, expected {expected_keypoints_length} (x,y,v for {NUM_KEYPOINTS} keypoints)\")\n",
    "            \n",
    "            # Only retain x and y values for keypoints\n",
    "            keypoints_2d = []\n",
    "            for i in range(0, min(len(keypoints), expected_keypoints_length), 3):\n",
    "                if keypoints[i+2] > 0:  # If visibility > 0 (keypoint is visible)\n",
    "                    keypoints_2d.append(keypoints[i])   # x\n",
    "                    keypoints_2d.append(keypoints[i+1]) # y\n",
    "                else:\n",
    "                    # For invisible points, still include them\n",
    "                    keypoints_2d.append(keypoints[i])   # x\n",
    "                    keypoints_2d.append(keypoints[i+1]) # y\n",
    "            \n",
    "            # Check if we have all keypoints\n",
    "            if len(keypoints_2d) == NUM_KEYPOINTS * 2:\n",
    "                keypoints_data[file_name] = keypoints_2d\n",
    "            else:\n",
    "                warnings.warn(f\"Image {file_name} has {len(keypoints_2d)//2} keypoints, expected {NUM_KEYPOINTS}. Skipping.\")\n",
    "\n",
    "    print(f\"Successfully loaded keypoints for {len(keypoints_data)} images.\")\n",
    "    return keypoints_data\n",
    "\n",
    "def create_filename_mapping(original_dir: Path, segmented_dir: Path) -> dict[str, str]:\n",
    "    \"\"\"Create a dictionary mapping original filenames to their corresponding segmented filenames\"\"\"\n",
    "    mapping = {}\n",
    "    original_files = [f.name for f in original_dir.glob('*') if f.is_file()]\n",
    "    print(f\"Found {len(original_files)} files in original directory.\")\n",
    "\n",
    "    count_found = 0\n",
    "    for orig_file in tqdm(original_files, desc=\"Mapping filenames\"):\n",
    "        seg_file = find_segmented_filename(orig_file, segmented_dir)\n",
    "        if seg_file:\n",
    "            mapping[orig_file] = seg_file\n",
    "            count_found += 1\n",
    "\n",
    "    print(f\"Created mapping for {count_found} out of {len(original_files)} original files.\")\n",
    "    if mapping:\n",
    "        sample_items = list(mapping.items())[:5]\n",
    "        print(\"Mapping examples:\")\n",
    "        for orig, seg in sample_items:\n",
    "            print(f\"  {orig} -> {seg}\")\n",
    "    else:\n",
    "         print(\"No mappings created. Check paths and filename patterns.\")\n",
    "    return mapping\n",
    "\n",
    "# --- Data Augmentation and Dataset ---\n",
    "\n",
    "def get_transforms(is_train: bool = True) -> A.Compose:\n",
    "    \"\"\"Get Albumentations transforms for training or validation\"\"\"\n",
    "    if is_train:\n",
    "        transform = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(limit=15, p=0.3, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.06, scale_limit=0.1, rotate_limit=0, p=0.3, border_mode=cv2.BORDER_CONSTANT, value=0), # No rotation here, handled above\n",
    "            A.Resize(height=TARGET_SIZE[0], width=TARGET_SIZE[1]),\n",
    "            A.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "            ToTensorV2(), # Converts image to tensor C, H, W and scales to [0, 1] if not already done by Normalize\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', label_fields=[], remove_invisible=False)) # Keypoints are (x, y)\n",
    "    else: # Validation/Test: Only resize and normalize\n",
    "        transform = A.Compose([\n",
    "            A.Resize(height=TARGET_SIZE[0], width=TARGET_SIZE[1]),\n",
    "            A.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "            ToTensorV2(),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', label_fields=[], remove_invisible=False))\n",
    "    return transform\n",
    "\n",
    "class TripleInputCattleDataset(Dataset):\n",
    "    def __init__(self, original_dir: Path, segmented_dir: Path, filenames: list[str],\n",
    "                 weights: list[float], keypoints_data: dict[str, list[float]],\n",
    "                 filename_map: dict[str, str], transform: A.Compose):\n",
    "        self.original_dir = original_dir\n",
    "        self.segmented_dir = segmented_dir\n",
    "        self.filenames = filenames\n",
    "        self.weights = np.array(weights) # Use numpy array for easier indexing\n",
    "        self.keypoints_data = keypoints_data\n",
    "        self.filename_map = filename_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def load_image(self, img_path: Path) -> np.ndarray | None:\n",
    "        if not img_path.is_file():\n",
    "            warnings.warn(f\"Image file not found: {img_path}\")\n",
    "            return None\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "             warnings.warn(f\"Failed to load image: {img_path}\")\n",
    "             return None\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB for transforms/models\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor] | None:\n",
    "        orig_filename = self.filenames[idx]\n",
    "        weight = self.weights[idx]\n",
    "\n",
    "        # --- Get image paths ---\n",
    "        orig_path = self.original_dir / orig_filename\n",
    "        seg_filename = self.filename_map.get(orig_filename)\n",
    "        if not seg_filename:\n",
    "             warnings.warn(f\"No segmented mapping for {orig_filename}. Skipping item.\")\n",
    "             # To make DataLoader work, we need to return something of the expected structure,\n",
    "             # but it will likely cause errors downstream or be filtered. Best to filter upstream.\n",
    "             # For simplicity, returning zero tensors, but filtering is better.\n",
    "             dummy_img = torch.zeros((3, TARGET_SIZE[0], TARGET_SIZE[1]), dtype=torch.float32)\n",
    "             dummy_kpts = torch.zeros(NUM_KEYPOINTS * 2, dtype=torch.float32)\n",
    "             dummy_weight = torch.tensor(0.0, dtype=torch.float32)\n",
    "             return dummy_img, dummy_img, dummy_kpts, dummy_weight\n",
    "\n",
    "        seg_path = self.segmented_dir / seg_filename\n",
    "\n",
    "        # --- Load images ---\n",
    "        orig_image = self.load_image(orig_path)\n",
    "        seg_image = self.load_image(seg_path)\n",
    "\n",
    "        if orig_image is None or seg_image is None:\n",
    "            # Handle case where image loading failed\n",
    "            warnings.warn(f\"Failed loading images for {orig_filename}. Skipping item.\")\n",
    "            dummy_img = torch.zeros((3, TARGET_SIZE[0], TARGET_SIZE[1]), dtype=torch.float32)\n",
    "            dummy_kpts = torch.zeros(NUM_KEYPOINTS * 2, dtype=torch.float32)\n",
    "            dummy_weight = torch.tensor(0.0, dtype=torch.float32)\n",
    "            return dummy_img, dummy_img, dummy_kpts, dummy_weight\n",
    "\n",
    "        # --- Load keypoints ---\n",
    "        keypoints_list_xy = [] # List of [x, y] tuples/lists\n",
    "        if orig_filename in self.keypoints_data:\n",
    "            kp_flat = self.keypoints_data[orig_filename]\n",
    "            for i in range(0, len(kp_flat), 2):\n",
    "                  keypoints_list_xy.append([kp_flat[i], kp_flat[i+1]]) # Use list format for albumentations\n",
    "        else:\n",
    "            # Handle missing keypoints if necessary, e.g., fill with zeros or center point\n",
    "            # For training, it's better to filter out samples without keypoints beforehand\n",
    "            warnings.warn(f\"No keypoints found for {orig_filename} in keypoints_data. Using zeros.\")\n",
    "            keypoints_list_xy = [[0.0, 0.0]] * NUM_KEYPOINTS\n",
    "\n",
    "\n",
    "        # --- Apply transformations ---\n",
    "        # Apply transform to original image and its keypoints\n",
    "        # Augmentations require keypoints relative to the image being augmented\n",
    "        try:\n",
    "            transformed = self.transform(image=orig_image, keypoints=keypoints_list_xy)\n",
    "            orig_tensor = transformed['image']\n",
    "            transformed_keypoints = transformed['keypoints'] # Keypoints are now relative to the transformed (resized) image\n",
    "\n",
    "            # For simplicity, we apply the same resize/norm transform to the segmented image.\n",
    "            # For complex *random* train transforms, more care is needed to ensure geometric consistency.\n",
    "            transformed_seg = self.transform(image=seg_image, keypoints=[]) # Apply same resize/norm\n",
    "            seg_tensor = transformed_seg['image']\n",
    "\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error during augmentation for {orig_filename}: {e}. Skipping.\")\n",
    "            dummy_img = torch.zeros((3, TARGET_SIZE[0], TARGET_SIZE[1]), dtype=torch.float32)\n",
    "            dummy_kpts = torch.zeros(NUM_KEYPOINTS * 2, dtype=torch.float32)\n",
    "            dummy_weight = torch.tensor(0.0, dtype=torch.float32)\n",
    "            return dummy_img, dummy_img, dummy_kpts, dummy_weight\n",
    "\n",
    "\n",
    "        # --- Normalize Keypoints ---\n",
    "        # Transformed keypoints are already relative to the output size (TARGET_SIZE)\n",
    "        # Normalize them to [0, 1] range based on TARGET_SIZE\n",
    "        keypoints_norm_flat = []\n",
    "        for x, y in transformed_keypoints:\n",
    "            norm_x = x / TARGET_SIZE[1] # width is target_size[1]\n",
    "            norm_y = y / TARGET_SIZE[0] # height is target_size[0]\n",
    "            keypoints_norm_flat.extend([norm_x, norm_y])\n",
    "\n",
    "        # Ensure correct length and clip values [0, 1]\n",
    "        if len(keypoints_norm_flat) != NUM_KEYPOINTS * 2:\n",
    "             warnings.warn(f\"Keypoint length mismatch after transform for {orig_filename} ({len(keypoints_norm_flat)}). Padding with zeros.\")\n",
    "             keypoints_norm_flat.extend([0.0] * (NUM_KEYPOINTS * 2 - len(keypoints_norm_flat)))\n",
    "             keypoints_norm_flat = keypoints_norm_flat[:NUM_KEYPOINTS * 2] # Truncate if too long\n",
    "\n",
    "        keypoints_norm_flat = [np.clip(v, 0.0, 1.0) for v in keypoints_norm_flat]\n",
    "        keypoints_tensor = torch.tensor(keypoints_norm_flat, dtype=torch.float32)\n",
    "\n",
    "        # --- Weight Tensor ---\n",
    "        weight_tensor = torch.tensor(weight, dtype=torch.float32)\n",
    "\n",
    "        return orig_tensor, seg_tensor, keypoints_tensor, weight_tensor\n",
    "\n",
    "# --- Enhanced Model Architecture ---\n",
    "\n",
    "class EnhancedTripleInputCattleWeightCNN(nn.Module):\n",
    "    def __init__(self, num_keypoints: int = NUM_KEYPOINTS, pretrained: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Original Image Branch (ResNet18) ---\n",
    "        self.orig_backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        num_ftrs_orig = self.orig_backbone.fc.in_features\n",
    "        self.orig_backbone.fc = nn.Identity() # Remove final classification layer\n",
    "        self.orig_bn = nn.BatchNorm1d(num_ftrs_orig)\n",
    "\n",
    "        # --- Segmented Image Branch (ResNet18) ---\n",
    "        self.seg_backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        num_ftrs_seg = self.seg_backbone.fc.in_features\n",
    "        self.seg_backbone.fc = nn.Identity()\n",
    "        self.seg_bn = nn.BatchNorm1d(num_ftrs_seg)\n",
    "\n",
    "        # --- Keypoints Branch ---\n",
    "        keypoints_input_dim = num_keypoints * 2 # x, y for each keypoint\n",
    "        self.keypoints_fc = nn.Sequential(\n",
    "            nn.Linear(keypoints_input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        keypoints_feature_dim = 256\n",
    "\n",
    "        # --- Combined Features Processing ---\n",
    "        combined_input_dim = num_ftrs_orig + num_ftrs_seg + keypoints_feature_dim\n",
    "        self.combined_fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(combined_input_dim), # BN before first linear layer\n",
    "            nn.Linear(combined_input_dim, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 1) # Output layer for weight regression\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, orig_img: torch.Tensor, seg_img: torch.Tensor, keypoints: torch.Tensor) -> torch.Tensor:\n",
    "        # Process images\n",
    "        orig_features = self.orig_backbone(orig_img)\n",
    "        orig_features = self.orig_bn(orig_features)\n",
    "\n",
    "        seg_features = self.seg_backbone(seg_img)\n",
    "        seg_features = self.seg_bn(seg_features)\n",
    "\n",
    "        # Process keypoints\n",
    "        keypoints_features = self.keypoints_fc(keypoints)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((orig_features, seg_features, keypoints_features), dim=1)\n",
    "\n",
    "        # Final prediction\n",
    "        weight = self.combined_fc(combined_features)\n",
    "        return weight\n",
    "\n",
    "# --- Training Function ---\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "                epochs: int, lr: float, weight_decay: float, device: torch.device,\n",
    "                model_save_path: Path):\n",
    "    \"\"\"Trains the model and saves the best version.\"\"\"\n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=lr * 0.4)\n",
    "\n",
    "    train_losses, val_losses, val_mae_list, val_rmse_list = [], [], [], []\n",
    "    best_val_rmse = float('inf')\n",
    "\n",
    "    print(f\"Starting training for {epochs} epochs on {device}...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", ncols=100)\n",
    "\n",
    "        for batch in train_pbar:\n",
    "            if batch is None: continue\n",
    "            orig_imgs, seg_imgs, keypoints, weights = batch\n",
    "            if keypoints.nelement() == 0: continue\n",
    "\n",
    "            orig_imgs, seg_imgs, keypoints, weights = (\n",
    "                orig_imgs.to(device),\n",
    "                seg_imgs.to(device),\n",
    "                keypoints.to(device),\n",
    "                weights.to(device).view(-1, 1),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(orig_imgs, seg_imgs, keypoints)\n",
    "            loss = criterion(outputs, weights)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds, all_targets = [], []\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Valid]\", ncols=100)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_pbar:\n",
    "                if batch is None: continue\n",
    "                orig_imgs, seg_imgs, keypoints, weights = batch\n",
    "                if keypoints.nelement() == 0: continue\n",
    "\n",
    "                orig_imgs, seg_imgs, keypoints, weights = (\n",
    "                    orig_imgs.to(device),\n",
    "                    seg_imgs.to(device),\n",
    "                    keypoints.to(device),\n",
    "                    weights.to(device).view(-1, 1),\n",
    "                )\n",
    "\n",
    "                outputs = model(orig_imgs, seg_imgs, keypoints)\n",
    "                loss = criterion(outputs, weights)\n",
    "                val_loss += loss.item()\n",
    "                val_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "                all_preds.extend(outputs.cpu().numpy().flatten())\n",
    "                all_targets.extend(weights.cpu().numpy().flatten())\n",
    "\n",
    "        epoch_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        mae = mean_absolute_error(all_targets, all_preds)\n",
    "        rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "        r2 = r2_score(all_targets, all_preds)\n",
    "        val_mae_list.append(mae)\n",
    "        val_rmse_list.append(rmse)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}, \"\n",
    "              f\"Val MAE: {mae:.2f} kg, Val RMSE: {rmse:.2f} kg, Val R2: {r2:.3f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        # Save best model based on validation RMSE\n",
    "        if rmse < best_val_rmse:\n",
    "            best_val_rmse = rmse\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"  => Saved best model to {model_save_path} (RMSE: {best_val_rmse:.2f} kg)\")\n",
    "\n",
    "    print(f\"Training completed. Best validation RMSE: {best_val_rmse:.2f} kg\")\n",
    "\n",
    "    # --- Plot training history ---\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss (Huber)')\n",
    "    plt.plot(val_losses, label='Validation Loss (Huber)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_mae_list, label='Validation MAE')\n",
    "    plt.plot(val_rmse_list, label='Validation RMSE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Error (kg)')\n",
    "    plt.legend()\n",
    "    plt.title('Validation Metrics')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('enhanced_training_history_B3.png')\n",
    "    print(\"Saved training history plot to enhanced_training_history_B3.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return train_losses, val_losses, val_mae_list, val_rmse_list\n",
    "\n",
    "\n",
    "def get_keypoints_from_image(image_path: Path, model=keypoint_detector_model, target_size: tuple[int, int]=TARGET_SIZE) -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    Get keypoints from an image using the pre-trained Keras keypoint detection model.\n",
    "    \"\"\"\n",
    "    print(f\"\\nPredicting keypoints for: {image_path}\")\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"Error: Keras keypoint model not loaded. Cannot predict keypoints.\")\n",
    "        return None\n",
    "        \n",
    "    if not Path(image_path).is_file():\n",
    "        print(f\"Error: Image file not found: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load the image\n",
    "    try:\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            print(f\"Error: Failed to load image: {image_path}\")\n",
    "            return None\n",
    "            \n",
    "        orig_height, orig_width = image.shape[:2]\n",
    "        print(f\"Original image size: {orig_width}x{orig_height}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Prepare image for model input\n",
    "    try:\n",
    "        image_resized = cv2.resize(image, (target_size[1], target_size[0]))  # CV2 takes (width, height)\n",
    "        image_input = image_resized / 255.0\n",
    "        image_input = np.expand_dims(image_input, axis=0)\n",
    "        print(f\"Model input shape: {image_input.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing image for prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Predict keypoints using the Keras model\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        keypoints_pred = model.predict(image_input, verbose=0)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Prediction completed in {elapsed:.4f} sec\")\n",
    "        print(f\"Model output shape: {keypoints_pred.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Keras model prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Process the keypoint predictions\n",
    "    try:\n",
    "        keypoints_relative_pixels = keypoints_pred[0].reshape(-1, 2)\n",
    "\n",
    "        if len(keypoints_relative_pixels) != NUM_KEYPOINTS:\n",
    "            print(f\"Error: Expected {NUM_KEYPOINTS} kps, got {len(keypoints_relative_pixels)}\")\n",
    "            keypoints_relative_pixels = keypoints_relative_pixels[:NUM_KEYPOINTS]\n",
    "            if len(keypoints_relative_pixels) < NUM_KEYPOINTS: return None\n",
    "\n",
    "        # --- Correctly scale keypoints to original resolution ---\n",
    "        target_h, target_w = target_size\n",
    "        scale_x = orig_width / target_w if target_w > 0 else 1.0\n",
    "        scale_y = orig_height / target_h if target_h > 0 else 1.0\n",
    "\n",
    "        keypoints_original = keypoints_relative_pixels * np.array([scale_x, scale_y])\n",
    "\n",
    "        # Clip coordinates to original image boundaries\n",
    "        keypoints_original[:, 0] = np.clip(keypoints_original[:, 0], 0, orig_width - 1)\n",
    "        keypoints_original[:, 1] = np.clip(keypoints_original[:, 1], 0, orig_height - 1)\n",
    "\n",
    "        print(f\"Predicted keypoints shape (original coords): {keypoints_original.shape}\")\n",
    "        print(f\"First few keypoints (original coords): {keypoints_original[:3]}\")\n",
    "\n",
    "        return keypoints_original\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing keypoint predictions: {e}\")\n",
    "        return None\n",
    "\n",
    "def segment_image_for_prediction(image_path: Path,\n",
    "                                 model=segmentation_model, \n",
    "                                 device=DEVICE, \n",
    "                                 seg_target_size: tuple[int, int] = SEGMENTATION_INPUT_SIZE,\n",
    "                                 debug_save_dir: Path = None) -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    Segments an image using the loaded PyTorch segmentation model.\n",
    "    \"\"\"\n",
    "    print(f\"Segmenting image for prediction: {image_path}\")\n",
    "    debug_filename_base = image_path.stem if debug_save_dir else None\n",
    "\n",
    "    if model is None:\n",
    "        print(\"Segmentation model not loaded. Returning None.\")\n",
    "        return None\n",
    "    \n",
    "    if not Path(image_path).is_file():\n",
    "        print(f\"Error: Image file not found: {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # 1. Load Image\n",
    "    try:\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is None:\n",
    "            print(f\"Error: Failed to load image: {image_path}\")\n",
    "            return None\n",
    "        orig_height, orig_width = image.shape[:2]\n",
    "        orig_image_bgr = image.copy()\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        print(f\"DEBUG: Original image dimensions: {orig_width}x{orig_height}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2. Preprocess using Albumentations\n",
    "    try:\n",
    "        seg_transform = A.Compose([\n",
    "            A.Resize(height=seg_target_size[0], width=seg_target_size[1]),\n",
    "            A.Normalize(mean=NORM_MEAN, std=NORM_STD),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        transformed = seg_transform(image=image_rgb)\n",
    "        image_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "        print(f\"DEBUG: Tensor shape for segmentation model: {image_tensor.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing for segmentation: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 3. Predict Segmentation Mask\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            output_logits = model(image_tensor) # (N, C, H, W) logits\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Segmentation completed in {elapsed:.4f} sec\")\n",
    "        print(f\"DEBUG: Segmentation model output shape (logits): {output_logits.shape}\")\n",
    "\n",
    "        # 4. Post-process using argmax\n",
    "        pred_mask = torch.argmax(output_logits, dim=1).squeeze().cpu().numpy().astype(np.uint8)\n",
    "        print(f\"DEBUG: Mask shape after argmax: {pred_mask.shape}\")\n",
    "        print(f\"DEBUG: Unique classes in predicted mask (at {seg_target_size}): {np.unique(pred_mask)}\")\n",
    "\n",
    "        if debug_save_dir and debug_filename_base:\n",
    "            debug_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            mask_viz = (pred_mask * (255 // max(1, np.max(pred_mask)) )).astype(np.uint8) if np.max(pred_mask) > 0 else pred_mask\n",
    "            cv2.imwrite(str(debug_save_dir / f\"{debug_filename_base}_class_mask_{seg_target_size[0]}x{seg_target_size[1]}.png\"), mask_viz)\n",
    "            print(f\"DEBUG: Saved class mask to {debug_save_dir / f'{debug_filename_base}_class_mask_{seg_target_size[0]}x{seg_target_size[1]}.png'}\")\n",
    "\n",
    "        # 5. Create binary mask for the COW (class 1) and resize\n",
    "        cow_class_index = 1 # Make sure this is the correct index for 'cow'\n",
    "        cow_mask_binary = (pred_mask == cow_class_index).astype(np.uint8) * 255\n",
    "        cow_mask_resized = cv2.resize(cow_mask_binary, (orig_width, orig_height), interpolation=cv2.INTER_NEAREST)\n",
    "        print(f\"DEBUG: Cow mask dimensions after resize: {cow_mask_resized.shape}\")\n",
    "        print(f\"DEBUG: Unique values in final cow mask (0 or 255): {np.unique(cow_mask_resized)}\")\n",
    "\n",
    "        if debug_save_dir and debug_filename_base:\n",
    "             cv2.imwrite(str(debug_save_dir / f\"{debug_filename_base}_cow_binary_mask_{orig_width}x{orig_height}.png\"), cow_mask_resized)\n",
    "             print(f\"DEBUG: Saved final binary cow mask\")\n",
    "\n",
    "        # 6. Apply mask to original image\n",
    "        mask_3channel = cv2.cvtColor(cow_mask_resized, cv2.COLOR_GRAY2BGR)\n",
    "        segmented_image = cv2.bitwise_and(orig_image_bgr, mask_3channel)\n",
    "\n",
    "        if debug_save_dir and debug_filename_base:\n",
    "            cv2.imwrite(str(debug_save_dir / f\"{debug_filename_base}_segmented_cow.png\"), segmented_image)\n",
    "            print(f\"DEBUG: Saved segmented cow image\")\n",
    "\n",
    "        return segmented_image # Return BGR image with mask applied\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during segmentation or post-processing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"Returning None as a fallback.\")\n",
    "        return None\n",
    "\n",
    "# --- Prediction Function ---\n",
    "\n",
    "def predict_weight(model: nn.Module, orig_img_path: Path, seg_img_input: Path | np.ndarray | None,\n",
    "                   keypoints_xy: np.ndarray, device: torch.device,\n",
    "                   target_size: tuple[int, int] = TARGET_SIZE) -> float | None:\n",
    "    model.eval()\n",
    "    transform = get_transforms(is_train=False)\n",
    "\n",
    "    try:\n",
    "        orig_image = cv2.imread(str(orig_img_path))\n",
    "        if orig_image is None:\n",
    "            print(f\"Prediction Error: Could not load original image {orig_img_path}\")\n",
    "            return None\n",
    "        orig_image_rgb = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        orig_height, orig_width = orig_image.shape[:2]\n",
    "\n",
    "        # --- Handle segmented input ---\n",
    "        seg_image = None\n",
    "        if isinstance(seg_img_input, Path): # If a path is provided\n",
    "            seg_image = cv2.imread(str(seg_img_input))\n",
    "            if seg_image is None:\n",
    "                print(f\"Warning: Could not load segmented image {seg_img_input}, attempting to segment on-the-fly...\")\n",
    "                seg_image = segment_image_for_prediction(orig_img_path)\n",
    "        \n",
    "        elif isinstance(seg_img_input, np.ndarray): # If an array is provided\n",
    "             seg_image = seg_img_input\n",
    "        \n",
    "        elif seg_img_input is None: # If None, perform segmentation\n",
    "            print(\"Segmented image not provided, performing automatic segmentation...\")\n",
    "            seg_image = segment_image_for_prediction(orig_img_path)\n",
    "        \n",
    "        if seg_image is None:\n",
    "            print(\"Error: Could not obtain a segmented image. Weight prediction is not possible.\")\n",
    "            return None\n",
    "\n",
    "        seg_image_rgb = cv2.cvtColor(seg_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Apply transforms to images\n",
    "        transformed_orig = transform(image=orig_image_rgb)\n",
    "        orig_tensor = transformed_orig['image'].unsqueeze(0).to(device)\n",
    "\n",
    "        transformed_seg = transform(image=seg_image_rgb)\n",
    "        seg_tensor = transformed_seg['image'].unsqueeze(0).to(device)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading/preprocessing images for weight prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if keypoints_xy is None:\n",
    "             print(\"Prediction Error: Keypoints are None\")\n",
    "             return None\n",
    "\n",
    "        if keypoints_xy.shape[0] != NUM_KEYPOINTS or keypoints_xy.shape[1] != 2:\n",
    "             print(f\"Prediction Error: Expected {NUM_KEYPOINTS} keypoints in (N, 2) format, got {keypoints_xy.shape}\")\n",
    "             # Attempt to fix shape, might not be robust\n",
    "             keypoints_xy = keypoints_xy[:NUM_KEYPOINTS, :2]\n",
    "             while len(keypoints_xy) < NUM_KEYPOINTS:\n",
    "                   keypoints_xy = np.vstack([keypoints_xy, [0, 0]]) # Pad\n",
    "\n",
    "        normalized_keypoints_flat = []\n",
    "        for x, y in keypoints_xy:\n",
    "            norm_x = x / orig_width\n",
    "            norm_y = y / orig_height\n",
    "            normalized_keypoints_flat.extend([norm_x, norm_y])\n",
    "\n",
    "        normalized_keypoints_flat = [np.clip(v, 0.0, 1.0) for v in normalized_keypoints_flat]\n",
    "        keypoints_tensor = torch.tensor(normalized_keypoints_flat, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing keypoints for weight prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- Predict Weight ---\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            predicted_weight = model(orig_tensor, seg_tensor, keypoints_tensor).item()\n",
    "        return predicted_weight\n",
    "    except Exception as e:\n",
    "        print(f\"Error during weight prediction: {e}\")\n",
    "        return None\n",
    "\n",
    "def visualize_prediction(orig_img_path: Path, keypoints: np.ndarray, predicted_weight: float):\n",
    "    \"\"\"Visualize the image with predicted keypoints and predicted weight.\"\"\"\n",
    "    try:\n",
    "        orig_image = cv2.imread(str(orig_img_path))\n",
    "        orig_image_rgb = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(orig_image_rgb)\n",
    "        \n",
    "        for kp in keypoints:\n",
    "            plt.scatter(kp[0], kp[1], c='red', s=20, marker='o')\n",
    "        \n",
    "        plt.title(f\"Predicted Weight: {predicted_weight:.2f} kg\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during visualization: {e}\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    original_dir = Path(r\"C:\\Users\\andrey\\.cache\\kagglehub\\datasets\\sadhliroomyprime\\cattle-weight-detection-model-dataset-12k\\versions\\3\\www.acmeai.tech Dataset - BMGF-LivestockWeight-CV\\Vector\\B3\\Side\\data\\images\")\n",
    "    segmented_dir = Path(r\"C:\\Users\\andrey\\.cache\\kagglehub\\datasets\\sadhliroomyprime\\cattle-weight-detection-model-dataset-12k\\versions\\3\\www.acmeai.tech Dataset - BMGF-LivestockWeight-CV\\Pixel\\B3\\annotations\")\n",
    "    csv_file = Path(r\"cow_weight_data.csv\")\n",
    "    json_annotation_file = Path(r\"coco_side_filtered.json\")\n",
    "\n",
    "    # --- Pre-checks ---\n",
    "    if not original_dir.is_dir():\n",
    "        print(f\"Error: Original image directory not found: {original_dir}\")\n",
    "        return\n",
    "    if not segmented_dir.is_dir():\n",
    "        print(f\"Error: Segmented image directory not found: {segmented_dir}\")\n",
    "        return\n",
    "    if not csv_file.is_file():\n",
    "        print(f\"Error: CSV file not found: {csv_file}\")\n",
    "        return\n",
    "    if not json_annotation_file.is_file():\n",
    "        print(f\"Error: JSON annotation file not found: {json_annotation_file}\")\n",
    "        return\n",
    "\n",
    "    # --- Load Data ---\n",
    "    print(\"Loading keypoints data from JSON file...\")\n",
    "    try:\n",
    "        keypoints_data = load_keypoints_data(json_annotation_file)\n",
    "        if not keypoints_data:\n",
    "            print(\"Error: No keypoints loaded. Check JSON file format and content.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading keypoints JSON: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Creating filename mapping between original and segmented images...\")\n",
    "    filename_map = create_filename_mapping(original_dir, segmented_dir)\n",
    "    if not filename_map:\n",
    "        print(\"Error: Could not map original to segmented filenames. Check directories and naming patterns.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading weights from CSV: {csv_file}\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df = df.dropna(subset=['weight', 'filename'])\n",
    "        df = df[df['weight'] > 0]\n",
    "        print(f\"Loaded {len(df)} rows from CSV after initial cleaning.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing CSV file: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Prepare Dataset ---\n",
    "    filenames_all, weights_all = [], []\n",
    "    print(\"Filtering dataset for available data (images, segmentation, keypoints)...\")\n",
    "    filtered_out_count = 0\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Filtering Data\"):\n",
    "        filename = str(row['filename'])\n",
    "        weight = float(row['weight'])\n",
    "\n",
    "        if (filename not in filename_map or\n",
    "            filename not in keypoints_data or\n",
    "            not (original_dir / filename).is_file() or\n",
    "            (filename_map.get(filename) and not (segmented_dir / filename_map[filename]).is_file())):\n",
    "            filtered_out_count += 1\n",
    "            continue\n",
    "        \n",
    "        filenames_all.append(filename)\n",
    "        weights_all.append(weight)\n",
    "\n",
    "    max_samples = 2700 \n",
    "    filenames_all = filenames_all[:max_samples]\n",
    "    weights_all = weights_all[:max_samples]\n",
    "\n",
    "    print(f\"Filtered dataset: {len(filenames_all)} samples remaining. ({filtered_out_count} removed).\")\n",
    "    if not filenames_all:\n",
    "        print(\"Error: No valid samples found after filtering. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # --- Split Data ---\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        filenames_all, weights_all, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(f\"Train samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "\n",
    "    # --- Create Datasets and DataLoaders ---\n",
    "    train_transform = get_transforms(is_train=True)\n",
    "    val_transform = get_transforms(is_train=False)\n",
    "\n",
    "    train_dataset = TripleInputCattleDataset(\n",
    "        original_dir, segmented_dir, X_train, y_train, keypoints_data, filename_map, transform=train_transform\n",
    "    )\n",
    "    val_dataset = TripleInputCattleDataset(\n",
    "        original_dir, segmented_dir, X_val, y_val, keypoints_data, filename_map, transform=val_transform\n",
    "    )\n",
    "\n",
    "    num_workers = 0 if os.name == 'nt' else 2\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    # --- Initialize Model ---\n",
    "    model = EnhancedTripleInputCattleWeightCNN(num_keypoints=NUM_KEYPOINTS, pretrained=True).to(DEVICE)\n",
    "\n",
    "    # --- Train Model ---\n",
    "    # Set to True to run training, False to skip and go to inference\n",
    "    RUN_TRAINING = False\n",
    "    if RUN_TRAINING:\n",
    "       print(\"Starting model training...\")\n",
    "       train_model(\n",
    "           model, train_loader, val_loader,\n",
    "           epochs=EPOCHS, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY,\n",
    "           device=DEVICE, model_save_path=Path(BEST_MODEL_SAVE_PATH)\n",
    "       )\n",
    "    else:\n",
    "       print(\"Skipping model training as per configuration.\")\n",
    "\n",
    "    # --- Inference Example ---\n",
    "    print(\"\\n--- Running Inference Example ---\")\n",
    "\n",
    "    if not Path(BEST_TRAIN_MODEL).is_file():\n",
    "        print(f\"Warning: Trained model file {BEST_TRAIN_MODEL} not found. Inference might not be meaningful.\")\n",
    "    else:\n",
    "        print(f\"Loading best model weights from {BEST_TRAIN_MODEL}\")\n",
    "        model.load_state_dict(torch.load(BEST_TRAIN_MODEL, map_location=DEVICE))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    mae_list_inference, all_actual_weights_inf, all_predicted_weights_inf = [], [], []\n",
    "\n",
    "    num_inference_samples = min(350, len(X_val))\n",
    "    if num_inference_samples == 0:\n",
    "        print(\"No samples available in validation set for inference.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"*\"*30)\n",
    "    print(\"WARNING: Inference uses keypoints predicted by the separate Keras model.\")\n",
    "    print(\"This differs from training, which used ground truth keypoints from JSON.\")\n",
    "    print(\"Performance might differ.\")\n",
    "    print(\"*\"*30 + \"\\n\")\n",
    "\n",
    "    for i in range(num_inference_samples):\n",
    "        idx = np.random.randint(0, len(X_val))\n",
    "        test_filename = X_val[idx]\n",
    "        actual_weight = y_val[idx]\n",
    "        orig_path = original_dir / test_filename\n",
    "        \n",
    "        print(f\"\\nPredicting for: {test_filename} (Sample {i+1}/{num_inference_samples})\")\n",
    "        print(f\"Actual weight: {actual_weight:.2f} kg\")\n",
    "\n",
    "        # 1. Get keypoints\n",
    "        predicted_keypoints_xy = get_keypoints_from_image(orig_path, model=keypoint_detector_model)\n",
    "        if predicted_keypoints_xy is None:\n",
    "            print(\"   -> Failed to predict keypoints for this image.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Predict weight, using automatic segmentation (seg_img_input=None)\n",
    "        predicted_weight = predict_weight(\n",
    "            model=model,\n",
    "            orig_img_path=orig_path,\n",
    "            seg_img_input=None, # Let predict_weight handle segmentation\n",
    "            keypoints_xy=predicted_keypoints_xy,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        \n",
    "        if predicted_weight is None:\n",
    "            print(\"   -> Failed to predict weight for this image.\")\n",
    "            continue\n",
    "\n",
    "        all_actual_weights_inf.append(actual_weight)\n",
    "        all_predicted_weights_inf.append(predicted_weight)\n",
    "\n",
    "        # Visualize the prediction\n",
    "        vis_seg_image = segment_image_for_prediction(orig_path, model=segmentation_model, device=DEVICE, debug_save_dir=Path(\"inference_visualizations/segmentation_debug\"))\n",
    "\n",
    "        if vis_seg_image is not None:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Original image with keypoints\n",
    "            plt.subplot(1, 2, 1)\n",
    "            orig_image_vis = cv2.cvtColor(cv2.imread(str(orig_path)), cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(orig_image_vis)\n",
    "            for kp_idx, kp in enumerate(predicted_keypoints_xy):\n",
    "                plt.scatter(kp[0], kp[1], c='red', s=30, marker='o')\n",
    "                plt.text(kp[0] + 5, kp[1] + 5, KEYPOINTS_NAMES[kp_idx % NUM_KEYPOINTS], color='white', backgroundcolor='red', fontsize=7)\n",
    "            plt.title(f\"Original Image\\nPredicted Weight: {predicted_weight:.2f} kg\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Segmented image\n",
    "            plt.subplot(1, 2, 2)\n",
    "            vis_seg_image_rgb = cv2.cvtColor(vis_seg_image, cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(vis_seg_image_rgb)\n",
    "            plt.title(f\"Segmented Image\\nActual Weight: {actual_weight:.2f} kg\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            vis_save_dir = Path(\"inference_visualizations\")\n",
    "            vis_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "            plt.savefig(vis_save_dir / f\"{test_filename}_prediction_visualization.png\")\n",
    "            print(f\"   -> Saved individual visualization to {vis_save_dir / f'{test_filename}_prediction_visualization.png'}\")\n",
    "            plt.close()\n",
    "        else:\n",
    "            visualize_prediction(orig_path, predicted_keypoints_xy, predicted_weight)\n",
    "        \n",
    "        error = abs(predicted_weight - actual_weight)\n",
    "        mae_list_inference.append(error)\n",
    "        print(f\"   Predicted weight: {predicted_weight:.2f} kg\")\n",
    "        print(f\"   Error: {error:.2f} kg\")\n",
    "\n",
    "    if mae_list_inference:\n",
    "        avg_mae_inf = sum(mae_list_inference) / len(mae_list_inference)\n",
    "        print(f\"\\nAverage MAE on {len(mae_list_inference)} inference samples: {avg_mae_inf:.2f} kg\")\n",
    "    else:\n",
    "        print(\"\\nNo successful inference predictions were made.\")\n",
    "\n",
    "    # --- Actual vs. Predicted Scatter Plot ---\n",
    "    if all_actual_weights_inf and all_predicted_weights_inf:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(all_actual_weights_inf, all_predicted_weights_inf, alpha=0.7, edgecolor='k', label=f'Predictions (MAE: {avg_mae_inf:.2f} kg)' if mae_list_inference else 'Predictions')\n",
    "        \n",
    "        min_val = min(min(all_actual_weights_inf, default=0), min(all_predicted_weights_inf, default=0))\n",
    "        data_max_val = max(max(all_actual_weights_inf, default=1), max(all_predicted_weights_inf, default=1))\n",
    "        graph_max_val = min(data_max_val, 300) \n",
    "        \n",
    "        plt.plot([min_val, graph_max_val], [min_val, graph_max_val], 'r--', lw=2, label='Ideal Prediction (y=x)')\n",
    "        plt.xlim(min_val, graph_max_val)\n",
    "        plt.ylim(min_val, graph_max_val)\n",
    "        \n",
    "        plt.xlabel(\"Actual Weight (kg)\", fontsize=12)\n",
    "        plt.ylabel(\"Predicted Weight (kg)\", fontsize=12)\n",
    "        plt.title(f\"Actual vs. Predicted Weights on Inference Set ({len(all_actual_weights_inf)} samples)\", fontsize=14)\n",
    "        plt.legend(fontsize=10)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        scatter_plot_path = Path(\"inference_visualizations\") / 'actual_vs_predicted_weights_scatter.png'\n",
    "        scatter_plot_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(scatter_plot_path)\n",
    "        print(f\"\\nSaved actual vs. predicted weights scatter plot to {scatter_plot_path}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"\\nNot enough data to generate actual vs. predicted weights scatter plot.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
